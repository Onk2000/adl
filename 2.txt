# Import TensorFlow library 
import tensorflow as tf
# Load the data # Load MNIST dataset 
'''
loads the MNIST dataset using the load_data() function provided by Keras, a high- level API of TensorFlow.
The MNIST dataset contains 70,000 images of handwritten digits that are split into 60,000 training images and 10,000 testing images.
'''
(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.mnist.load_data()
# Preprocess the data
# Reshape and normalize training data
train_data = train_data.reshape((60000, 784)) / 255.0 # Reshape and normalize testing data
test_data = test_data.reshape((10000, 784)) / 255.0 # Convert training labels to one-hot encoding
train_labels = tf.keras.utils.to_categorical(train_labels) # Convert testing labels to one-hot encoding
test_labels = tf.keras.utils.to_categorical(test_labels)
# Define the model architecture
model = tf.keras.models.Sequential([ # Define sequential model
#Add a fully connected layer with 128 units, ReLU activation, and L2 regularization
tf.keras.layers.Dense(128, activation='relu', input_shape=(784,), kernel_regularizer=tf.keras.regularizers.l2(0.01)),
# Add another fully connected layer with 64 units,ReLU activation, and L2 regularization
tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras. regularizers.l2(0.01)),
# Add a final output layer with 10 units (one for each class), softmax activation
tf.keras.layers.Dense(10, activation='softmax')
])
# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),# Use Adam optimizer with learning rate 0.001
loss='categorical_crossentropy',# Use categorical cross-entropy loss function 
metrics=['accuracy']) # Monitor accuracy during training
history = model.fit (train_data, train_labels, epochs=10, batch_size=128, # Train the model for 10 epochs, using batches of size 128, and validate on the testing data at the end of each epoch
validation_data= (test_data, test_labels))
